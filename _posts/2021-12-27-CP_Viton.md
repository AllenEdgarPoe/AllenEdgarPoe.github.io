---
title: Toward Characteristic-Preserving Image-based Virtual Try-On Network 
excerpt: VTON 논문 2번째

categories:
    - deep-learning
tags:
    - VTON
  
comments: true
use_math: true
---

> [CP-VITON](https://arxiv.org/pdf/1807.07688.pdf) 
> - ECCV 2018 ACCEPTED

VITON의 후속작 격인 논문이다. 이전 VITON은 shape context matching 을 통해서 spatial deformation 은 해결하려고 했지만, 몸의 형태와 옷을 제대로 맞춰서 융합시키지 못했기 때문에 퍼포먼스가 다소 떨어졌다. 그래서 위 논문에서는 옷의 로고, 주름 등 보다 더 자세한 특징을 잡을 수 있는 Characteristic-Preserving Image-based Virtual Try-On NEtwork (CP-VTON)를 제시한다. 두 개의 모듈로 되어있는데 (a) Geometric Matching Module 과 (b) Try-On Module이 있다. 

<img src="https://user-images.githubusercontent.com/43398106/147446564-17377e0f-d6d9-4302-be1e-53384c128883.png">

$c_i$를 입고 있는 레퍼런스 이미지 $I_i$ 에 target clothing 인 $c_o$를 입혀서 $I_o$라는 이미지를 만든느 것이 해당 논문의 태스크 이다. 그냥 생각해봤을 때, ($I_i$, $c_o$, $I_o$) 이 세 가지를 모두 가지고 있으면, ground truth가 있으므로 제일 쉽게 학습시킬 수 있다. 하지만 이런 3가지 정보를 모두 가지고 있는게 어렵기 때문에, 앞선 연구 에서는 ($I_i$, $c_i$) 이 두 가지 정보를 가지고 학습시켜서 $f(I_i, c_i)$ = $I_i$ 를 할 수 있게끔 한다. 하지만 이러면 identity mapping 이 되버리니까 직접적으로 $I_i$ 를 인풋으로 사용하지 않고 대신 person representation *p* 를 사용한다.

여기서 Person representation *p* 는 총 3개의 정보를 갖게 된다. 
* Pose Heatmap: *"Realtime multiperson 2d pose estimation using part affinity fields"* 에서 나온 pose estimator 로 18개의 키포인트 정보를 담은 히트맵이다. 18 채널을 가지고 있다. 
* Human body representation: *" Look into person: Self-supervised structure-sensitive learning and a new
benchmark for human parsing."* 에서 나온 human parser 로 대략적인 몸통의 형태를 0과 1로 표현한 매트릭스 를 얻는다. 
* Face and Hair: 앞에 사용했던 human parser로 얼굴과 머리카락 정보를 받는다. 

---

(a) GMM (Geometric Matching Module)
in-shop 이미지 (c) 와 input image representation (p) 를 align하는 것을 의미한다. 그래서 사람 형태에 맞는 warped clothes를 만들어서 다음 모듈에 넘어가게 된다. warped clothes를 만드는 $\theta$ 는 spatial transformation parameter로 TPS (Thin-Plate Spline) transformation 모듈의 파라미터라고 생각하면 된다. 그래서 이렇게 나온 아웃풋을 가지고 기존 사람이 입고 있는 옷 이미지와 L1_loss로 계산하면 된다. 

$L_{GMM}(\theta) = ||c'-c_t||_1 = ||T_\theta)(c) - c_t||_1$

(b) Try-on Module
