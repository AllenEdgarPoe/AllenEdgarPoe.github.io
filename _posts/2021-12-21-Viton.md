---
title: VITON, an Image-based Virtual Try-on Network 
excerpt: Virtual Try-On 첫 번째 논문을 읽어보자

categories:
    - deep-learning
tags:
    - VTON
  
comments: true
use_math: true
---

> [VITON](https://arxiv.org/pdf/1711.08447.pdf) 
> - CVPR 2018 ACCEPTED

# 2D Image-based Virtual Try-on Network

Virtual Try-on 논문을 접한 첫 번째 논문이다. 누가 VITON이 좀 '짜치다' 라고 표현했는데 무슨 뜻인지 알거 같다. 엄청 이래저래 모델을 붙여서 만들었다. (Image-inpainting 같은 GAN 모델들이 좀 그런 케이스가 많다.) 그래서 솔직히 말하면 좀 엄청 좋은 논문 같지는 않고, 처음 2D 이미지를 가지고 옷을 입혔다는 면에서 contribution 이 있는 것 같다. 그래서 아주 간략하게 포스팅 하려고 한다..

---
<img src="https://user-images.githubusercontent.com/43398106/146874595-cd911304-9efe-4fe7-8b11-3a7554961e12.png" width="500">

## Virtual Try-On 이란? 
위에 그림처럼 사람 이미지와 옷 이미지가 주어졌을 때, 사람의 포즈나 각도 등 다른 요소는 유지하면서 옷만 바뀌어 입도록 하는 task 다. 

## An Overview of VITON 

<img src="https://user-images.githubusercontent.com/43398106/146875025-3723a15d-cdd8-43d9-a6f7-a1917de68eb9.png" >

Input으로는 Target Clothing *c* 와 Person Representation *p* 가 주어진다. *p*는 사람 이미지에서 추출한 3개의 정보이고 이들을 행으로 묶어서 Coarse network *G_c* 를 통과해 보다 막연한 이미지 *I*' 와 clothing mask *M* 을 생성한다. 그리고 이를 다시 Refinement network *G_r* 를 통과시켜 보다 더 샤프하고 정교한 이미지를 만들어낸다. 

## 1. Person Representation
<img src="https://user-images.githubusercontent.com/43398106/146875230-5ad4d314-68a1-4f79-8751-09abacbb5018.png" >

Input 으로 들어갈 *p* 는 총 3개의 정보를 갖게 된다. 
* Pose Heatmap: *"Realtime multiperson 2d pose estimation using part affinity fields"* 에서 나온 pose estimator 로 18개의 키포인트 정보를 담은 히트맵이다. 18 채널을 가지고 있다. 
* Human body representation: *" Look into person: Self-supervised structure-sensitive learning and a new
benchmark for human parsing."* 에서 나온 human parser 로 대략적인 몸통의 형태를 0과 1로 표현한 매트릭스 를 얻는다. 
* Face and Hair: 앞에 사용했던 human parser로 얼굴과 머리카락 정보를 받는다. 

이 3개의 정보를 channel-wise 로 concatenate를 시켜서 256 x 192 x 22 의 정보로 만든다. 

## 2. Coarse Generator 
앞에 만들었던 *p* 와 의류 이미지 *c* 를 *G_c* 를 통과시켜서 대략적인 이미지 *I'* 와 의류 마스크 *M* 을 만든다. 이때 *G_c* 는 Encoder-Decoder 형태이고 U-net 구조로 만들었다. 목적함수는 

$
L_Gc = \sum_{i=5}^{5}\lambda_i \lVert \phi(I') - \phi(I) \rVert _1 + \lVert M - M_0 \rVert _1
$

인데, 여기서 $\phi$는 VGG19 로 만든 feature extractor 라고 할 수 있다. vgg network 에서 5개의 레이어를 골라서 각 레이어마다 perceptual loss를 구하는 식으로 만든다. 

## 3. Warped Clothing item 
<img src="https://user-images.githubusercontent.com/43398106/146878428-fdbbf7c9-91c9-476b-b698-bb3644072650.png">

2 에서 clothing Mask M을 얻었다. 이 마스크와 원래 인풋으로 들어왔던 Target clothing *c* 와의 차이를 줄여서 최대한 옷의 디테일을 살려야 한다. 그런데 이 마스크는 사람이 입은 상태에서 만들어진 것이기 때문에 *c* 와는 차이가 크다. 따라서 사람의 포즈나 형태 등의 정보를 가미시켜서 warped clothing *c'* 을 새로 만들었다. 이때 사용한게 TPS Transformation 이라는 것인데 대충 설명하자면 타겟 옷인 *c* 와 *M* 간의 거리를 구한다음 그 거리만큼 Shape Context 라고 가정해서 와핑 시키는 것이다. 

## 4. Refinement Network 

그래서 대충 만들어진 이미지인 *I'* 와 옷의 마스크 인 *M* 이 refinement network 인 *G_r* 에 통과해 결과물을 만든다. *G_r* 의 아웃풋은 1 채널의 composition mask 인 $\alpha$ 로 두 개의 source 인 *I'* 와 *M* 에서 얼마만큼의 정보나 반영하는지를 나타내는 지표라고 생각하면 된다. 

$I'' = \alpha * c' + (1-\alpha) * I'$ <br>
$L_Gr = L_{perc}(I'', I) - \lambda_{warp}\lVert \alpha \rVert _1 + \lambda_{TV} \lVert \nabla\alpha \rVert_1$

위가 Try-on image (I'') 이고 밑에는 목적함수 인데, 여기서 $\lambda$ 값 두 개는 각각 어떤 노름에 가중치를 더 둘 것인가 에 대한 것이라고 생각하면 된다. $\lVert \nabla\alpha \rVert _1$ 는 total variance norm 으로 composition mask $\alpha$ 의 그래디언트에 패널티를 주는 방식이여서 스무스 한 영역을 만들 수 있도록 도와준다. 자연스러운 이미지를 만들기 위해서는 이런 regularizer를 사용하는 것이 도움이 된다고 한다. 

## 5. Results
<img src="https://user-images.githubusercontent.com/43398106/146880257-275a1a9a-54c3-444e-8c0e-26ee7c741896.png">
각 다른 GAN 모델과 비교했을 때 좀 더 자연스러운 이미지가 나타난다. 하지만 두 번째 행 처럼 상의만 바뀌어야하는데 하의도 같이 바뀌는 등의 문제점이 보이긴 한다. 

* Failure Cases 

<img src="https://user-images.githubusercontent.com/43398106/146880475-8420cd68-f321-4f67-8914-c03a20fe3ccd.png">

옷의 일부분이 팔 등으로 많이 가려지거나, 아니면 원래 입은 옷의 형태가 새로운 옷의 형태랑 많이 다를 경우는 제대로 생성하지 못하는 것 같다.

---

꽤 쉬운 논문인데, 쓰여진 것과 다르게 오만 네트워크를 다 가져다가 써서 솔직히 지저분하다. 결과값이 만족스러우면 감안해야 하는거지만.. 좀 지저분한게 용납이 안 된다. ㅜ 암튼 더 공부해볼지는 논문 몇 개 더 읽어보고 결정해야 겠다.